{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35606c97-e178-4328-9ccc-933338d2e42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f893eed2-c59d-481c-be93-732d6a5f3bd5",
   "metadata": {},
   "source": [
    "### getting class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4904344-abe3-478f-b21a-69b1b1ec7f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 90 files belonging to 4 classes.\n",
      "['Canna_Indice', 'Canna_Indice_Dead_Leafs', 'Egyptian_White_Guava', 'Noni_Fruit']\n"
     ]
    }
   ],
   "source": [
    "validation_set = tf.keras.utils.image_dataset_from_directory(\n",
    "    'validation',\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=None,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=32,\n",
    "    image_size=(128, 128),\n",
    "    shuffle=True,\n",
    "    seed=None,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    "    interpolation=\"bilinear\",\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False\n",
    ")\n",
    "class_name = validation_set.class_names\n",
    "print(class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cdad93-13d7-4bda-9d5b-a00abfdaa094",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "680d081f-8c48-4730-beba-a5047b81d207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 517ms/step\n",
      "Prediction for Canna_Indice_Dead_Leafs_31.jpeg: Canna_Indice_Dead_Leafs (95.61%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
      "Prediction for Canna_Indice_Dead_Leafs_54.jpeg: Canna_Indice_Dead_Leafs (92.25%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
      "Prediction for Canna_Indice_Dead_Leafs_97.jpeg: Canna_Indice (45.2%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
      "Prediction for Canna_Indice_Dead_Leafs_9.jpeg: Canna_Indice_Dead_Leafs (96.36%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\n",
      "Prediction for Canna_Indice_Dead_Leafs_55.jpeg: Canna_Indice_Dead_Leafs (99.79%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n",
      "Prediction for Canna_Indice_Dead_Leafs_14.jpeg: Canna_Indice_Dead_Leafs (90.14%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
      "Prediction for Canna_Indice_Dead_Leafs_48.jpeg: Canna_Indice_Dead_Leafs (60.51%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "Prediction for Canna_Indice_Dead_Leafs_72.jpeg: Canna_Indice_Dead_Leafs (90.94%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step\n",
      "Prediction for Canna_Indice_Dead_Leafs_2.jpeg: Canna_Indice_Dead_Leafs (77.56%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
      "Prediction for Canna_Indice_Dead_Leafs_90.jpeg: Canna_Indice_Dead_Leafs (69.85%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "Prediction for Canna_Indice_Dead_Leafs_45.jpeg: Canna_Indice_Dead_Leafs (98.95%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "Prediction for Canna_Indice_Dead_Leafs_105.jpeg: Canna_Indice_Dead_Leafs (93.88%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step\n",
      "Prediction for Egyptian_White_Guava_98.jpeg: Egyptian_White_Guava (98.65%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
      "Prediction for Egyptian_White_Guava_20.jpeg: Egyptian_White_Guava (99.7%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
      "Prediction for Egyptian_White_Guava_16.jpeg: Egyptian_White_Guava (65.68%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "Prediction for Egyptian_White_Guava_40.jpeg: Egyptian_White_Guava (99.52%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "Prediction for Egyptian_White_Guava_17.jpeg: Egyptian_White_Guava (99.83%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "Prediction for Egyptian_White_Guava_7.jpeg: Egyptian_White_Guava (98.8%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
      "Prediction for Egyptian_White_Guava_48.jpeg: Egyptian_White_Guava (99.05%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      "Prediction for Egyptian_White_Guava_72.jpeg: Egyptian_White_Guava (99.9%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      "Prediction for Egyptian_White_Guava_75.jpeg: Egyptian_White_Guava (99.76%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "Prediction for Egyptian_White_Guava_79.jpeg: Egyptian_White_Guava (99.97%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step\n",
      "Prediction for Noni_Fruit_99.jpeg: Noni_Fruit (93.99%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n",
      "Prediction for Noni_Fruit_17.jpeg: Noni_Fruit (89.88%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
      "Prediction for Noni_Fruit_16.jpeg: Noni_Fruit (97.61%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "Prediction for Noni_Fruit_77.jpeg: Noni_Fruit (93.33%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step\n",
      "Prediction for Noni_Fruit_93.jpeg: Noni_Fruit (91.89%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n",
      "Prediction for Noni_Fruit_11.jpeg: Noni_Fruit (97.05%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
      "Prediction for Noni_Fruit_3.jpeg: Noni_Fruit (94.06%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step\n",
      "Prediction for Noni_Fruit_2.jpeg: Noni_Fruit (97.64%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n",
      "Prediction for Noni_Fruit_105.jpeg: Noni_Fruit (93.33%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242ms/step\n",
      "Prediction for Noni_Fruit_64.jpeg: Noni_Fruit (96.51%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "Prediction for Noni_Fruit_43.jpeg: Noni_Fruit (85.49%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
      "Prediction for Noni_Fruit_59.jpeg: Noni_Fruit (86.27%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "Prediction for Canna_Indice_37.jpeg: Canna_Indice (94.5%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step\n",
      "Prediction for Canna_Indice_27.jpeg: Canna_Indice (96.68%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "Prediction for Canna_Indice_92.jpeg: Canna_Indice (98.7%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n",
      "Prediction for Canna_Indice_71.jpeg: Canna_Indice (94.92%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n",
      "Prediction for Canna_Indice_88.jpeg: Canna_Indice (51.67%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      "Prediction for Canna_Indice_53.jpeg: Canna_Indice (86.27%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
      "Prediction for Canna_Indice_86.jpeg: Canna_Indice (98.19%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "Prediction for Canna_Indice_2.jpeg: Canna_Indice (92.44%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      "Prediction for Canna_Indice_90.jpeg: Noni_Fruit (50.44%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
      "Prediction for Canna_Indice_116.jpeg: Canna_Indice (97.18%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
      "Prediction for Canna_Indice_32.jpeg: Canna_Indice (99.51%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "Prediction for Canna_Indice_42.jpeg: Canna_Indice (98.75%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
      "Prediction for Canna_Indice_15.jpeg: Canna_Indice (83.43%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "Prediction for Canna_Indice_9.jpeg: Canna_Indice (99.12%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
      "Prediction for Canna_Indice_18.jpeg: Canna_Indice (54.45%)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "Prediction for Canna_Indice_96.jpeg: Canna_Indice (99.81%)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Load the TensorFlow model once globally\n",
    "model_path = \"/Users/mohammedalqadda/PlantCare/trained_plantCare_model.keras\"\n",
    "model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "# Define the prediction function\n",
    "def model_prediction(test_image):\n",
    "    # Load and preprocess the image\n",
    "    image = tf.keras.preprocessing.image.load_img(test_image, target_size=(256, 256))\n",
    "    input_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    input_arr = input_arr # Normalize image data to [0,1]\n",
    "    input_arr = np.expand_dims(input_arr, axis=0)  # Convert single image to batch\n",
    "\n",
    "    # Get predictions\n",
    "    predictions = model.predict(input_arr)\n",
    "    return predictions\n",
    "\n",
    "# Set the directory containing your test images\n",
    "image_dir = \"/Users/mohammedalqadda/PlantCare/test\"\n",
    "\n",
    "# List all files in the directory\n",
    "for root, dirs, files in os.walk(image_dir):\n",
    "    for file_name in files:\n",
    "        if file_name.endswith(\".jpeg\"):\n",
    "            file_path = os.path.join(root, file_name)\n",
    "\n",
    "            # Get the prediction for the image\n",
    "            pred = model_prediction(file_path)\n",
    "            result_index = np.argmax(pred)  # Get the index of the highest probability\n",
    "            pred_percentage = np.round(pred[0][result_index] * 100, 2)  # Get prediction percentage\n",
    "\n",
    "            # Define class names\n",
    "            class_names = ['Canna_Indice', 'Canna_Indice_Dead_Leafs', 'Egyptian_White_Guava', 'Noni_Fruit']\n",
    "            \n",
    "            # Print the prediction and confidence\n",
    "            print(f\"Prediction for {file_name}: {class_names[result_index]} ({pred_percentage}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54505b5f-b1b1-4f6c-b094-757dd668d33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step\n",
      "Prediction for Canna_Indice_Dead_Leafs_31.jpeg: Canna_Indice_Dead_Leafs (95.61%) - True Class: Canna_Indice\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step\n",
      "Prediction for Canna_Indice_Dead_Leafs_54.jpeg: Canna_Indice_Dead_Leafs (92.25%) - True Class: Canna_Indice\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n",
      "Prediction for Canna_Indice_Dead_Leafs_97.jpeg: Canna_Indice (45.2%) - True Class: Canna_Indice\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "Prediction for Canna_Indice_Dead_Leafs_9.jpeg: Canna_Indice_Dead_Leafs (96.36%) - True Class: Canna_Indice\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n",
      "Prediction for Canna_Indice_Dead_Leafs_55.jpeg: Canna_Indice_Dead_Leafs (99.79%) - True Class: Canna_Indice\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n",
      "Prediction for Canna_Indice_Dead_Leafs_14.jpeg: Canna_Indice_Dead_Leafs (90.14%) - True Class: Canna_Indice\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n",
      "Prediction for Canna_Indice_Dead_Leafs_48.jpeg: Canna_Indice_Dead_Leafs (60.51%) - True Class: Canna_Indice\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "Prediction for Canna_Indice_Dead_Leafs_72.jpeg: Canna_Indice_Dead_Leafs (90.94%) - True Class: Canna_Indice\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "Prediction for Canna_Indice_Dead_Leafs_2.jpeg: Canna_Indice_Dead_Leafs (77.56%) - True Class: Canna_Indice\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n",
      "Prediction for Canna_Indice_Dead_Leafs_90.jpeg: Canna_Indice_Dead_Leafs (69.85%) - True Class: Canna_Indice\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n",
      "Prediction for Canna_Indice_Dead_Leafs_45.jpeg: Canna_Indice_Dead_Leafs (98.95%) - True Class: Canna_Indice\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      "Prediction for Canna_Indice_Dead_Leafs_105.jpeg: Canna_Indice_Dead_Leafs (93.88%) - True Class: Canna_Indice\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
      "Prediction for Egyptian_White_Guava_98.jpeg: Egyptian_White_Guava (98.65%) - True Class: Egyptian_White_Guava\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
      "Prediction for Egyptian_White_Guava_20.jpeg: Egyptian_White_Guava (99.7%) - True Class: Egyptian_White_Guava\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n",
      "Prediction for Egyptian_White_Guava_16.jpeg: Egyptian_White_Guava (65.68%) - True Class: Egyptian_White_Guava\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
      "Prediction for Egyptian_White_Guava_40.jpeg: Egyptian_White_Guava (99.52%) - True Class: Egyptian_White_Guava\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      "Prediction for Egyptian_White_Guava_17.jpeg: Egyptian_White_Guava (99.83%) - True Class: Egyptian_White_Guava\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
      "Prediction for Egyptian_White_Guava_7.jpeg: Egyptian_White_Guava (98.8%) - True Class: Egyptian_White_Guava\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
      "Prediction for Egyptian_White_Guava_48.jpeg: Egyptian_White_Guava (99.05%) - True Class: Egyptian_White_Guava\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "Prediction for Egyptian_White_Guava_72.jpeg: Egyptian_White_Guava (99.9%) - True Class: Egyptian_White_Guava\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step\n",
      "Prediction for Egyptian_White_Guava_75.jpeg: Egyptian_White_Guava (99.76%) - True Class: Egyptian_White_Guava\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      "Prediction for Egyptian_White_Guava_79.jpeg: Egyptian_White_Guava (99.97%) - True Class: Egyptian_White_Guava\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "Prediction for Noni_Fruit_99.jpeg: Noni_Fruit (93.99%) - True Class: Noni_Fruit\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step\n",
      "Prediction for Noni_Fruit_17.jpeg: Noni_Fruit (89.88%) - True Class: Noni_Fruit\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
      "Prediction for Noni_Fruit_16.jpeg: Noni_Fruit (97.61%) - True Class: Noni_Fruit\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "Prediction for Noni_Fruit_77.jpeg: Noni_Fruit (93.33%) - True Class: Noni_Fruit\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step\n",
      "Prediction for Noni_Fruit_93.jpeg: Noni_Fruit (91.89%) - True Class: Noni_Fruit\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step\n",
      "Prediction for Noni_Fruit_11.jpeg: Noni_Fruit (97.05%) - True Class: Noni_Fruit\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "Prediction for Noni_Fruit_3.jpeg: Noni_Fruit (94.06%) - True Class: Noni_Fruit\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "Prediction for Noni_Fruit_2.jpeg: Noni_Fruit (97.64%) - True Class: Noni_Fruit\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
      "Prediction for Noni_Fruit_105.jpeg: Noni_Fruit (93.33%) - True Class: Noni_Fruit\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n",
      "Prediction for Noni_Fruit_64.jpeg: Noni_Fruit (96.51%) - True Class: Noni_Fruit\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n",
      "Prediction for Noni_Fruit_43.jpeg: Noni_Fruit (85.49%) - True Class: Noni_Fruit\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "Prediction for Noni_Fruit_59.jpeg: Noni_Fruit (86.27%) - True Class: Noni_Fruit\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "Prediction for Canna_Indice_37.jpeg: Canna_Indice (94.5%) - True Class: Canna_Indice\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step\n",
      "Prediction for Canna_Indice_27.jpeg: Canna_Indice (96.68%) - True Class: Canna_Indice\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
      "Prediction for Canna_Indice_92.jpeg: Canna_Indice (98.7%) - True Class: Canna_Indice\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 512ms/step\n",
      "Prediction for Canna_Indice_71.jpeg: Canna_Indice (94.92%) - True Class: Canna_Indice\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
      "Prediction for Canna_Indice_88.jpeg: Canna_Indice (51.67%) - True Class: Canna_Indice\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "Prediction for Canna_Indice_53.jpeg: Canna_Indice (86.27%) - True Class: Canna_Indice\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
      "Prediction for Canna_Indice_86.jpeg: Canna_Indice (98.19%) - True Class: Canna_Indice\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "Prediction for Canna_Indice_2.jpeg: Canna_Indice (92.44%) - True Class: Canna_Indice\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
      "Prediction for Canna_Indice_90.jpeg: Noni_Fruit (50.44%) - True Class: Canna_Indice\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
      "Prediction for Canna_Indice_116.jpeg: Canna_Indice (97.18%) - True Class: Canna_Indice\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
      "Prediction for Canna_Indice_32.jpeg: Canna_Indice (99.51%) - True Class: Canna_Indice\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n",
      "Prediction for Canna_Indice_42.jpeg: Canna_Indice (98.75%) - True Class: Canna_Indice\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
      "Prediction for Canna_Indice_15.jpeg: Canna_Indice (83.43%) - True Class: Canna_Indice\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step\n",
      "Prediction for Canna_Indice_9.jpeg: Canna_Indice (99.12%) - True Class: Canna_Indice\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
      "Prediction for Canna_Indice_18.jpeg: Canna_Indice (54.45%) - True Class: Canna_Indice\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step\n",
      "Prediction for Canna_Indice_96.jpeg: Canna_Indice (99.81%) - True Class: Canna_Indice\n",
      "\n",
      "Overall Accuracy: 76.00% (38/50)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Load the TensorFlow model once globally\n",
    "model_path = \"/Users/mohammedalqadda/PlantCare/trained_plantCare_model.keras\"\n",
    "model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "# Define the prediction function\n",
    "def model_prediction(test_image):\n",
    "    # Load and preprocess the image\n",
    "    image = tf.keras.preprocessing.image.load_img(test_image, target_size=(256, 256))\n",
    "    input_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    input_arr = input_arr  # Normalize image data to [0,1]\n",
    "    input_arr = np.expand_dims(input_arr, axis=0)  # Convert single image to batch\n",
    "\n",
    "    # Get predictions\n",
    "    predictions = model.predict(input_arr)\n",
    "    return predictions\n",
    "\n",
    "# Class names (ensure this matches the names used in your file structure)\n",
    "class_names = ['Canna_Indice', 'Canna_Indice_Dead_Leafs', 'Egyptian_White_Guava', 'Noni_Fruit']\n",
    "\n",
    "# Set the directory containing your test images\n",
    "image_dir = \"/Users/mohammedalqadda/PlantCare/test\"\n",
    "\n",
    "# Initialize counters for accuracy\n",
    "correct_predictions = 0\n",
    "total_images = 0\n",
    "\n",
    "# List all files in the directory\n",
    "for root, dirs, files in os.walk(image_dir):\n",
    "    for file_name in files:\n",
    "        if file_name.endswith(\".jpeg\"):\n",
    "            file_path = os.path.join(root, file_name)\n",
    "\n",
    "            # Get the prediction for the image\n",
    "            pred = model_prediction(file_path)\n",
    "            result_index = np.argmax(pred)  # Get the index of the highest probability\n",
    "            pred_percentage = np.round(pred[0][result_index] * 100, 2)  # Get prediction percentage\n",
    "            predicted_class = class_names[result_index]\n",
    "\n",
    "            # Extract the true class from the file name (assuming the class name is in the file name)\n",
    "            true_class = None\n",
    "            for class_name in class_names:\n",
    "                if class_name in file_name:\n",
    "                    true_class = class_name\n",
    "                    break\n",
    "\n",
    "            # Check if the prediction is correct\n",
    "            if true_class == predicted_class:\n",
    "                correct_predictions += 1\n",
    "\n",
    "            total_images += 1\n",
    "\n",
    "            # Print the prediction and confidence\n",
    "            print(f\"Prediction for {file_name}: {predicted_class} ({pred_percentage}%) - True Class: {true_class}\")\n",
    "\n",
    "# Calculate and print overall accuracy\n",
    "if total_images > 0:\n",
    "    accuracy = (correct_predictions / total_images) * 100\n",
    "    print(f\"\\nOverall Accuracy: {accuracy:.2f}% ({correct_predictions}/{total_images})\")\n",
    "else:\n",
    "    print(\"No images found for testing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2454d0-9865-4879-a070-ff114e672a16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
